{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d342461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceb83e4",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae025aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with shape: (7821, 2)\n",
      "Columns: ['judgement', 'summary']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_data(dataset_path, delimiter=\",\"):\n",
    "    try:\n",
    "        dataset = pd.read_csv(dataset_path, sep=delimiter, quoting=1)\n",
    "        print(f\"Loaded dataset with shape: {dataset.shape}\")\n",
    "        print(f\"Columns: {list(dataset.columns)}\\n\")\n",
    "        return dataset\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "dataset_path = \"datasets/legal.csv\"\n",
    "df = load_data(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed00353",
   "metadata": {},
   "source": [
    "## Remove Duplicate Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36df8616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (7821, 2)\n",
      "Duplicate rows: 77\n",
      "Dataset after removing duplicates: (7744, 2)\n",
      "Rows removed: 77\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_duplicates(dataset):\n",
    "    print(f\"Original dataset shape: {dataset.shape}\")\n",
    "    print(f\"Duplicate rows: {dataset.duplicated().sum()}\")\n",
    "    \n",
    "    df_unique = dataset[~dataset.duplicated(keep='first')]\n",
    "    \n",
    "    print(f\"Dataset after removing duplicates: {df_unique.shape}\")\n",
    "    print(f\"Rows removed: {dataset.shape[0] - df_unique.shape[0]}\\n\")\n",
    "    \n",
    "    return df_unique\n",
    "\n",
    "df_clean = remove_duplicates(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1296ec1",
   "metadata": {},
   "source": [
    "## Remove NaN Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02a57076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (7744, 2)\n",
      "Rows with NaN: 0\n",
      "Dataset after removing NaN: (7744, 2)\n",
      "Rows removed: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_nan_values(dataset):\n",
    "    \"\"\"Remove rows with NaN values\"\"\"\n",
    "    print(f\"Original dataset shape: {dataset.shape}\")\n",
    "    print(f\"Rows with NaN: {dataset.isna().any(axis=1).sum()}\")\n",
    "    \n",
    "    df_clean = dataset.dropna()\n",
    "    \n",
    "    print(f\"Dataset after removing NaN: {df_clean.shape}\")\n",
    "    print(f\"Rows removed: {dataset.shape[0] - df_clean.shape[0]}\\n\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "df_clean = remove_nan_values(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a80fb1",
   "metadata": {},
   "source": [
    "## Identify and Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "feecc9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (7744, 2)\n",
      "\n",
      "judgement:\n",
      "  Q1: 12028, Q3: 32527, IQR: 20498\n",
      "  Bounds: [-18719, 63275]\n",
      "  Outliers removed: 675\n",
      "\n",
      "summary:\n",
      "  Q1: 2425, Q3: 5444, IQR: 3019\n",
      "  Bounds: [-2104, 9972]\n",
      "  Outliers removed: 275\n",
      "\n",
      "Dataset after removing outliers: (6794, 2)\n",
      "Total rows removed: 950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_outliers_iqr(dataset, text_columns=['judgement', 'summary']):\n",
    "    print(f\"Original dataset shape: {dataset.shape}\\n\")\n",
    "    \n",
    "    for col in text_columns:\n",
    "        if col in dataset.columns:\n",
    "            dataset[f'{col}_length'] = dataset[col].astype(str).apply(len)\n",
    "    \n",
    "    df_filtered = dataset.copy()\n",
    "    for col in text_columns:\n",
    "        if f'{col}_length' in df_filtered.columns:\n",
    "            Q1 = df_filtered[f'{col}_length'].quantile(0.25)\n",
    "            Q3 = df_filtered[f'{col}_length'].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            \n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            outliers_before = df_filtered.shape[0]\n",
    "            df_filtered = df_filtered[\n",
    "                (df_filtered[f'{col}_length'] >= lower_bound) & \n",
    "                (df_filtered[f'{col}_length'] <= upper_bound)\n",
    "            ]\n",
    "            \n",
    "            print(f\"{col}:\")\n",
    "            print(f\"  Q1: {Q1:.0f}, Q3: {Q3:.0f}, IQR: {IQR:.0f}\")\n",
    "            print(f\"  Bounds: [{lower_bound:.0f}, {upper_bound:.0f}]\")\n",
    "            print(f\"  Outliers removed: {outliers_before - df_filtered.shape[0]}\\n\")\n",
    "    \n",
    "    df_filtered = df_filtered.drop(columns=[col for col in df_filtered.columns if col.endswith('_length')])\n",
    "    \n",
    "    print(f\"Dataset after removing outliers: {df_filtered.shape}\")\n",
    "    print(f\"Total rows removed: {dataset.shape[0] - df_filtered.shape[0]}\\n\")\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "df_clean = remove_outliers_iqr(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b735284c",
   "metadata": {},
   "source": [
    "## Save Cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6805e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Cleaned dataset saved to: datasets/legal_cleaned.csv\n",
      "  Total rows: 6794\n",
      "  Total columns: 2\n"
     ]
    }
   ],
   "source": [
    "def save_cleaned_dataset(dataset, output_path, delimiter=\",\"):\n",
    "    dataset.to_csv(\n",
    "        output_path, \n",
    "        sep=delimiter, \n",
    "        index=False, \n",
    "        quoting=csv.QUOTE_ALL\n",
    "    )\n",
    "    print(f\"✓ Cleaned dataset saved to: {output_path}\")\n",
    "    print(f\"  Total rows: {len(dataset)}\")\n",
    "    print(f\"  Total columns: {len(dataset.columns)}\")\n",
    "\n",
    "output_path = \"datasets/legal_cleaned.csv\"\n",
    "save_cleaned_dataset(df_clean, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63c86f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CLEANED DATASET SUMMARY\n",
      "============================================================\n",
      "Final shape: (6794, 2)\n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.DataFrame'>\n",
      "Index: 6794 entries, 4 to 7819\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype\n",
      "---  ------     --------------  -----\n",
      " 0   judgement  6794 non-null   str  \n",
      " 1   summary    6794 non-null   str  \n",
      "dtypes: str(2)\n",
      "memory usage: 157.1 MB\n",
      "None\n",
      "\n",
      "First few rows:\n",
      "                                            judgement  \\\n",
      "4   This appeal raises the issue as to whether a t...   \n",
      "6   This appeal concerns the proper ambit of the o...   \n",
      "7   In August 2007, the vessel B Atlantic, owned b...   \n",
      "8   Part II of the Landlord and Tenant Act 1954 co...   \n",
      "12  This appeal raises a question relating to the ...   \n",
      "\n",
      "                                              summary  \n",
      "4   This appeal concerns the extent to which a non...  \n",
      "6   Ahava was a shop in Covent Garden, London, whi...  \n",
      "7   In August 2007 B Atlantic (the Vessel), owned ...  \n",
      "8   This appeal concerns qualified security of ten...  \n",
      "12  The Appellant, Mr OBrien, is a retired self em...  \n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CLEANED DATASET SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Final shape: {df_clean.shape}\")\n",
    "print(f\"\\nDataset Info:\")\n",
    "print(df_clean.info())\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df_clean.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
